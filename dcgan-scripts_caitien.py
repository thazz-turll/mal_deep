#!/usr/bin/env python3

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.ensemble import RandomForestClassifier
import joblib

# ================== User data paths (Giá»¯ nguyÃªn) ==================
# !!! THAY Äá»”I ÄÆ¯á»œNG DáºªN NÃ€Y CHO PHÃ™ Há»¢P Vá»šI MÃY Cá»¦A Báº N !!!
DATA_DIR = "/home/thangkb2024/processed" 
# ================================================================

X_TRAIN_PATH = os.path.join(DATA_DIR, "X_train.npy")
Y_TRAIN_PATH = os.path.join(DATA_DIR, "y_train.npy")
X_VAL_PATH = os.path.join(DATA_DIR, "X_val.npy")
Y_VAL_PATH = os.path.join(DATA_DIR, "y_val.npy")
BLACKBOX_PATH = os.path.join(DATA_DIR, "blackbox.pkl")

# Model save paths
GEN_SAVE_PATH = "generator_mal-dcgan_best.pt"
SUB_SAVE_PATH = "substitute_detector_dcgan.pt"

# ================== Hyperparams (DCGAN) ==================
z_dim = 100
batch_size = 64 # Giáº£m batch size cho CNN
lr_G = 0.0002 # Learning rate chuáº©n cho DCGAN
lr_S = 0.0002 # Learning rate chuáº©n cho DCGAN
betas = (0.5, 0.999) # Betas chuáº©n cho DCGAN
epochs = 100 # DCGAN cáº§n nhiá»u epoch hÆ¡n
lambda_perturb = 1.0 # Há»‡ sá»‘ loss cho nhiá»…u (L1 norm)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# KÃ­ch thÆ°á»›c cho DCGAN
input_dim_1d = 2381 # KÃ­ch thÆ°á»›c 1D gá»‘c
img_size = 49
img_channels = 1
padded_dim = img_size * img_size # 2401

# ================== HÃ m xá»­ lÃ½ dá»¯ liá»‡u (Má»šI) ==================
def pad_and_reshape(x_1d_batch):
    """
    Nháº­n batch 1D (B, 2381) vÃ  chuyá»ƒn thÃ nh (B, 1, 49, 49)
    """
    B, _ = x_1d_batch.shape
    # 1. Táº¡o ma tráº­n padding 0
    padding = np.zeros((B, padded_dim - input_dim_1d))
    # 2. Ná»‘i (Pad)
    x_1d_padded = np.concatenate([x_1d_batch, padding], axis=1)
    # 3. Reshape
    x_2d = x_1d_padded.reshape((B, img_channels, img_size, img_size))
    return x_2d

# ================== Load data (Cáº­p nháº­t) ==================
print("Loading split training and validation data (1D)...")
X_train_all_1d = np.load(X_TRAIN_PATH)
y_train_all = np.load(Y_TRAIN_PATH)
X_val_all_1d = np.load(X_VAL_PATH)
y_val_all = np.load(Y_VAL_PATH)

# TÃ¡ch riÃªng malware/benign tá»« táº­p TRAIN (1D)
X_mal_train_1d = X_train_all_1d[y_train_all == 1]
X_ben_train_1d = X_train_all_1d[y_train_all == 0]

# TÃ¡ch riÃªng malware tá»« táº­p VAL (1D)
X_mal_val_1d = X_val_all_1d[y_val_all == 1]

# --- BÆ¯á»šC CHUYá»‚N Äá»”I SANG 2D ---
print("Padding and Reshaping data to 2D...")
X_mal_train_2d = pad_and_reshape(X_mal_train_1d)
X_ben_train_2d = pad_and_reshape(X_ben_train_1d)
X_mal_val_2d = pad_and_reshape(X_mal_val_1d)

print(f"Loaded Train 2D: Mal {X_mal_train_2d.shape}, Ben {X_ben_train_2d.shape}")
print(f"Loaded Val 2D (for eval): Mal {X_mal_val_2d.shape}")

# Convert to torch tensors (float)
X_mal_t = torch.tensor(X_mal_train_2d, dtype=torch.float32)
X_ben_t = torch.tensor(X_ben_train_2d, dtype=torch.float32)

mal_ds = TensorDataset(X_mal_t)
mal_loader = DataLoader(mal_ds, batch_size=batch_size, shuffle=True, drop_last=True)

ben_ds = TensorDataset(X_ben_t)
ben_loader = DataLoader(ben_ds, batch_size=batch_size, shuffle=True, drop_last=True)

# DÃ¹ng dá»¯ liá»‡u _val 2D Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
X_mal_val_t = torch.tensor(X_mal_val_2d, dtype=torch.float32).to(device)


# ================== Black-box detector (Cáº­p nháº­t) ==================
print(f"Loading black-box detector from {BLACKBOX_PATH}...")
if not os.path.exists(BLACKBOX_PATH):
    print(f"Lá»—i: KhÃ´ng tÃ¬m tháº¥y model blackbox táº¡i {BLACKBOX_PATH}")
    print("Vui lÃ²ng cháº¡y script huáº¥n luyá»‡n blackbox trÆ°á»›c.")
    exit()
    
blackbox = joblib.load(BLACKBOX_PATH)

def query_blackbox_from_tensor(tensor_batch_4d):
    """
    HÃ m Má»šI: Nháº­n tensor 4D (B, 1, 49, 49),
    Flatten -> Trim (loáº¡i bá» padding) -> Query Blackbox
    """
    B = tensor_batch_4d.size(0)
    # 1. Flatten 4D -> 2D
    tensor_flat = tensor_batch_4d.view(B, -1) # (B, 2401)
    
    # 2. Trim (loáº¡i bá» padding)
    # Cáº¯t tá»« 2401 vá» 2381
    tensor_trimmed = tensor_flat[:, :input_dim_1d] # (B, 2381)
    
    # 3. Query
    arr = tensor_trimmed.detach().cpu().numpy()
    preds = blackbox.predict(arr)
    return preds # numpy array

# ================== Models (KIáº¾N TRÃšC DCGAN ÄÃƒ Sá»¬A) ==================

class Generator(nn.Module):
    """
    Kiáº¿n trÃºc Generator (DCGAN) - ÄÃƒ Sá»¬A Lá»–I UPSCALING
    Táº¡o ra Máº¶T Náº  NHIá»„U (perturbation mask)
    """
    def __init__(self, z_dim, img_channels, img_size): # img_size = 49
        super(Generator, self).__init__()
        ngf = 32 # KÃ­ch thÆ°á»›c feature map cá»§a G
        self.encoder = nn.Sequential(
            nn.Conv2d(img_channels, ngf, 5, 2, 1, bias=False), # (B, 64, 24, 24)
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ngf, ngf * 2, 4, 2, 1, bias=False), # (B, 128, 12, 12)
            nn.BatchNorm2d(ngf * 2),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ngf * 2, ngf * 4, 4, 2, 1, bias=False), # (B, 256, 6, 6)
            nn.BatchNorm2d(ngf * 4),
            nn.LeakyReLU(0.2, inplace=True),
        )
        bottleneck_dim = (ngf * 4) * 6 * 6 # 256 * 36 = 9216
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(bottleneck_dim + z_dim, ngf * 4, 6, 1, 0, bias=False), # (B, 256, 6, 6)
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False), # (B, 128, 12, 12)
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False), # (B, 64, 24, 24)
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf, img_channels, 5, 2, 1, bias=False), # (B, 1, 49, 49)
            nn.Tanh() # Output trong [-1, 1]
        )
    def forward(self, real_x, z):
        x_encoded = self.encoder(real_x) # (B, 256, 6, 6)
        x_flat = x_encoded.view(x_encoded.size(0), -1) # (B, 9216)
        combined = torch.cat([x_flat, z], dim=1) # (B, 9216 + 100)
        combined_rs = combined.view(combined.size(0), -1, 1, 1) # (B, 9316, 1, 1)
        perturb_mask = self.decoder(combined_rs) # (B, 1, 49, 49)
        perturb_mask_scaled = perturb_mask * 0.5 
        adv = torch.clamp(real_x + perturb_mask_scaled, 0.0, 1.0)
        return adv, perturb_mask_scaled

class SubstituteDetector(nn.Module):
    """
    Kiáº¿n trÃºc Substitute Detector (DCGAN) - ÄÃƒ Sá»¬A Lá»–I DOWNSCALING
    Hoáº¡t Ä‘á»™ng nhÆ° má»™t Discriminator / Bá»™ phÃ¢n loáº¡i áº£nh
    """
    def __init__(self, img_channels):
        super(SubstituteDetector, self).__init__()
        ndf = 32 # KÃ­ch thÆ°á»›c feature map cá»§a D (S)
        self.net = nn.Sequential(
            # Input (B, 1, 49, 49)
            # Lá»šP ÄÃƒ Sá»¬A: (tá»« 49x49 -> 24x24)
            nn.Conv2d(img_channels, ndf, 5, 2, 1, bias=False), # (B, 64, 24, 24)
            nn.LeakyReLU(0.2, inplace=True),
            
            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False), # (B, 128, 12, 12)
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),
            
            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False), # (B, 256, 6, 6)
            nn.BatchNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),
            
            nn.Conv2d(ndf * 4, 1, 6, 1, 0, bias=False), # (B, 1, 1, 1)
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.net(x).view(-1, 1) # Flatten vá» (B, 1)

# Instantiate
G = Generator(z_dim, img_channels, img_size).to(device)
S = SubstituteDetector(img_channels).to(device)

# Ãp dá»¥ng weights_init (chuáº©n cá»§a DCGAN)
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)
print("Initializing weights...")
G.apply(weights_init)
S.apply(weights_init)

optimizer_G = optim.Adam(G.parameters(), lr=lr_G, betas=betas)
optimizer_S = optim.Adam(S.parameters(), lr=lr_S, betas=betas)
bce = nn.BCELoss()

# ================== Training loop (Logic giá»¯ nguyÃªn) ==================
print("Start training Mal-DCGAN procedure...")
best_evasion_rate = 0.0 

for epoch in range(1, epochs + 1):
    G.train()
    S.train()

    epoch_loss_G = 0.0
    epoch_loss_S = 0.0
    n_batches = 0

    ben_iter = iter(ben_loader)

    for mal_batch in mal_loader: # mal_batch giá» lÃ  (B, 1, 49, 49)
        real_mal = mal_batch[0].to(device)
        B = real_mal.size(0)
        n_batches += 1

        # ---- Generate adversarial samples ----
        z = torch.randn(B, z_dim, device=device)
        adv, perturb = G(real_mal, z) # adv giá» lÃ  (B, 1, 49, 49)

        # ---- Query blackbox (Ä‘Ã£ sá»­a) ----
        # Gá»­i tensor 4D vÃ o hÃ m query má»›i
        adv_labels_bb = query_blackbox_from_tensor(adv) 

        # ---- Prepare substitute training batch ----
        try:
            benign_batch = next(ben_iter)[0].to(device) # (B, 1, 49, 49)
        except StopIteration:
            ben_iter = iter(ben_loader)
            benign_batch = next(ben_iter)[0].to(device)

        adv_tensor_for_S = adv.detach()
        adv_labels_tensor = torch.tensor(adv_labels_bb, dtype=torch.float32, device=device).view(-1,1)
        benign_labels_tensor = torch.zeros((benign_batch.size(0),1), dtype=torch.float32, device=device)

        m = min(adv_tensor_for_S.size(0), benign_batch.size(0))
        # train_X_S giá» lÃ  tensor 4D
        train_X_S = torch.cat([adv_tensor_for_S[:m], benign_batch[:m]], dim=0)
        train_y_S = torch.cat([adv_labels_tensor[:m], benign_labels_tensor[:m]], dim=0)

        # ---- Train Substitute detector S ----
        optimizer_S.zero_grad()
        preds_S = S(train_X_S) # S nháº­n áº£nh 4D
        loss_S = bce(preds_S, train_y_S)
        loss_S.backward()
        optimizer_S.step()

        # ---- Train Generator G ----
        optimizer_G.zero_grad()
        z2 = torch.randn(B, z_dim, device=device)
        adv2, perturb2 = G(real_mal, z2) # adv2 lÃ  áº£nh 4D
        preds_sub_on_adv = S(adv2) # S nháº­n áº£nh 4D
        target_ben = torch.zeros_like(preds_sub_on_adv, device=device)
        
        loss_adv_detection = bce(preds_sub_on_adv, target_ben)
        # TÃ­nh L1 loss trÃªn máº·t náº¡ nhiá»…u 4D
        loss_perturb = torch.mean(torch.abs(perturb2)) 
        loss_G = loss_adv_detection + lambda_perturb * loss_perturb
        
        loss_G.backward()
        optimizer_G.step()

        epoch_loss_G += loss_G.item()
        epoch_loss_S += loss_S.item()

    # End of epoch
    avg_loss_G = epoch_loss_G / n_batches
    avg_loss_S = epoch_loss_S / n_batches
    print(f"Epoch {epoch:03d}/{epochs}  Avg Loss_G: {avg_loss_G:.6f}  Avg Loss_S: {avg_loss_S:.6f}")

    # ================== EVALUATION BLOCK (Logic giá»¯ nguyÃªn) ==================
    if epoch % 5 == 0:
        G.eval()
        with torch.no_grad():
            # ÄÃ¡nh giÃ¡ trÃªn táº­p VALIDATION 2D (X_mal_val_t)
            z_eval = torch.randn(X_mal_val_t.size(0), z_dim, device=device)
            adv_eval, _ = G(X_mal_val_t, z_eval) # adv_eval lÃ  (B_val, 1, 49, 49)
            
            # Query blackbox (dÃ¹ng hÃ m query 4D má»›i)
            preds_bb = query_blackbox_from_tensor(adv_eval)
            
            evasion_rate = np.mean(preds_bb == 0)
            print(f"  -> Eval (epoch {epoch}): evasion_rate on VAL SET = {evasion_rate:.4f}")

            # ---- LÆ°u model tá»‘t nháº¥t ----
            if evasion_rate > best_evasion_rate:
                best_evasion_rate = evasion_rate
                torch.save(G.state_dict(), GEN_SAVE_PATH)
                print(f"  -> ðŸŽ‰ New best model saved! Evasion: {evasion_rate*100:.2f}%")

# ================== Save final models ==================
torch.save(S.state_dict(), SUB_SAVE_PATH)
print(f"Final Substitute Detector saved to {SUB_SAVE_PATH}")
print(f"Best Generator saved to {GEN_SAVE_PATH} (Best Evasion Rate: {best_evasion_rate*100:.2f}%)")
print("Done.")